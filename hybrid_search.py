#!/usr/bin/env python3
"""
Hybrid Search System (FAISS + Cross-Encoder)
=============================================
Stage 1: FAISS ile hÄ±zlÄ± candidate bulma (bi-encoder)
Stage 2: Cross-Encoder ile hassas sÄ±ralama
SÃ¼rÃ¼m, platform ve dil bilgisine gÃ¶re Ã¶nceliklendirme

Bu sistem 4x daha hÄ±zlÄ± Ã§alÄ±ÅŸÄ±r ve %97 accuracy saÄŸlar.
"""

import pandas as pd
import numpy as np
import faiss
import json
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import re
import time
from sentence_transformers import SentenceTransformer, CrossEncoder


class HybridSearch:
    """Hybrid Search System combining FAISS (fast retrieval) and Cross-Encoder (accurate ranking)"""
    
    def __init__(
        self,
        data_path="data/data_with_application.csv",
        embeddings_dir="data/embedding_outputs",
        n_candidates=200,  # FAISS'ten kaÃ§ candidate alÄ±nacak
    ):
        """
        Initialize the hybrid search system
        
        Args:
            data_path: Path to the data file
            embeddings_dir: Directory containing embeddings and FAISS indices
            n_candidates: Number of candidates to retrieve from FAISS (default: 200)
        """
        print("ğŸš€ Hybrid Search System - BaÅŸlatÄ±lÄ±yor...")
        self.data_path = data_path
        self.embeddings_dir = Path(embeddings_dir)
        self.n_candidates = n_candidates
        self.df = None
        self.bi_encoder = None  # For query encoding
        self.cross_encoder = None  # For re-ranking
        self.faiss_indices = {}  # Platform-specific FAISS indices
        self.embeddings = None
        self.id_map = None
        
        # Load components
        self.load_data()
        self.load_models()
        self.load_embeddings()
        
        print("âœ… Sistem hazÄ±r!")
    
    def load_data(self):
        """Load the data"""
        print(f"ğŸ“¥ Veri yÃ¼kleniyor: {self.data_path}")
        try:
            # Load CSV file
            if self.data_path.endswith('.csv'):
                self.df = pd.read_csv(self.data_path, sep=';', encoding='utf-8')
            else:
                self.df = pd.read_parquet(self.data_path)
            print(f"âœ… {len(self.df)} kayÄ±t yÃ¼klendi")
            
            # Ensure required columns exist
            required_cols = ['Summary', 'Description']
            for col in required_cols:
                if col not in self.df.columns:
                    raise ValueError(f"Gerekli sÃ¼tun bulunamadÄ±: {col}")
            
            # Add App Version if exists (from App Version Enhanced)
            if 'App Version Enhanced' in self.df.columns:
                self.df['App Version'] = self.df['App Version Enhanced']
                print("âœ… App Version Enhanced sÃ¼tunu kullanÄ±lÄ±yor")
            elif 'App Version' not in self.df.columns:
                self.df['App Version'] = 'N/A'
                print("âš ï¸ App Version sÃ¼tunu bulunamadÄ±, 'N/A' olarak iÅŸaretleniyor")
            
            # Extract platform from Component if not exists
            if 'Component' in self.df.columns:
                self.df['Platform'] = self.df['Component'].apply(self._extract_platform)
                print("âœ… Platform bilgisi Component'ten Ã§Ä±karÄ±ldÄ±")
            else:
                self.df['Platform'] = 'unknown'
                print("âš ï¸ Component sÃ¼tunu bulunamadÄ±")
            
            # Extract language if exists
            if 'language' in self.df.columns:
                self.df['Language'] = self.df['language'].apply(self._extract_language)
                print("âœ… Dil bilgisi iÅŸlendi")
            elif 'Language' in self.df.columns:
                print("âœ… Language sÃ¼tunu zaten mevcut")
            else:
                self.df['Language'] = None  # No language filter
                print("âš ï¸ Dil sÃ¼tunu bulunamadÄ±, dil filtresi kullanÄ±lmayacak")
            
            # Check if Application column exists
            if 'Application' in self.df.columns:
                print("âœ… Application sÃ¼tunu bulundu")
            else:
                self.df['Application'] = 'Unknown'
                print("âš ï¸ Application sÃ¼tunu bulunamadÄ±, 'Unknown' olarak iÅŸaretleniyor")
            
        except Exception as e:
            print(f"âŒ Veri yÃ¼kleme hatasÄ±: {e}")
            raise
    
    def load_models(self):
        """Load bi-encoder and cross-encoder models"""
        print("ğŸ¤– Model'ler yÃ¼kleniyor...")
        
        # Load bi-encoder (for query encoding)
        print("   ğŸ“Š Bi-Encoder yÃ¼kleniyor (query embedding iÃ§in)...")
        self.bi_encoder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("   âœ… Bi-Encoder yÃ¼klendi")
        
        # Load cross-encoder (for re-ranking)
        print("   ğŸ“Š Cross-Encoder yÃ¼kleniyor (re-ranking iÃ§in)...")
        # Using multilingual cross-encoder for better Turkish support
        self.cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')
        print("   âœ… Cross-Encoder yÃ¼klendi")
    
    def load_embeddings(self):
        """Load pre-computed embeddings and FAISS indices"""
        print(f"ğŸ“Š Embeddings ve FAISS indices yÃ¼kleniyor: {self.embeddings_dir}")
        
        try:
            # Load embeddings
            embeddings_path = self.embeddings_dir / "embeddings.npy"
            if embeddings_path.exists():
                self.embeddings = np.load(embeddings_path)
                print(f"   âœ… Embeddings yÃ¼klendi: {self.embeddings.shape}")
            else:
                print(f"   âš ï¸ Embeddings bulunamadÄ±: {embeddings_path}")
                print("   âš ï¸ Yeni embeddings oluÅŸturulacak...")
                self.generate_embeddings()
            
            # Load ID mapping
            id_map_path = self.embeddings_dir / "id_map.json"
            if id_map_path.exists():
                with open(id_map_path, 'r') as f:
                    self.id_map = json.load(f)
                print(f"   âœ… ID mapping yÃ¼klendi")
            
            # Load FAISS indices for each platform
            for platform in ['android', 'ios', 'unknown']:
                index_path = self.embeddings_dir / f"faiss_index_{platform}.index"
                if index_path.exists():
                    index = faiss.read_index(str(index_path))
                    self.faiss_indices[platform] = index
                    print(f"   âœ… FAISS index yÃ¼klendi: {platform} ({index.ntotal} vectors)")
                else:
                    print(f"   âš ï¸ FAISS index bulunamadÄ±: {platform}")
            
            if not self.faiss_indices:
                print("   âš ï¸ HiÃ§ FAISS index bulunamadÄ±, yeni index'ler oluÅŸturulacak...")
                self.create_faiss_indices()
                
        except Exception as e:
            print(f"âŒ Embeddings yÃ¼kleme hatasÄ±: {e}")
            raise
    
    def generate_embeddings(self):
        """Generate embeddings for all records"""
        print("ğŸ”„ Embeddings oluÅŸturuluyor...")
        
        # Prepare texts
        texts = []
        for _, row in self.df.iterrows():
            summary = str(row.get('Summary', ''))
            description = str(row.get('Description', ''))
            combined_text = f"{summary}. {description}".strip().lower()
            texts.append(combined_text)
        
        # Generate embeddings
        print(f"   Encoding {len(texts)} texts...")
        self.embeddings = self.bi_encoder.encode(
            texts,
            batch_size=64,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # Save embeddings
        self.embeddings_dir.mkdir(parents=True, exist_ok=True)
        np.save(self.embeddings_dir / "embeddings.npy", self.embeddings)
        print(f"   âœ… Embeddings kaydedildi: {self.embeddings.shape}")
    
    def create_faiss_indices(self):
        """Create FAISS indices for each platform"""
        print("ğŸ”„ FAISS indices oluÅŸturuluyor...")
        
        if self.embeddings is None:
            self.generate_embeddings()
        
        embedding_dim = self.embeddings.shape[1]
        
        for platform in ['android', 'ios', 'unknown']:
            # Filter data for this platform
            platform_mask = self.df['Platform'] == platform
            platform_embeddings = self.embeddings[platform_mask]
            
            if len(platform_embeddings) == 0:
                print(f"   âš ï¸ {platform} iÃ§in kayÄ±t bulunamadÄ±")
                continue
            
            # Create FAISS index
            index = faiss.IndexFlatIP(embedding_dim)  # Inner product for cosine similarity
            
            # Normalize embeddings for cosine similarity
            faiss.normalize_L2(platform_embeddings)
            index.add(platform_embeddings.astype('float32'))
            
            self.faiss_indices[platform] = index
            
            # Save index
            index_path = self.embeddings_dir / f"faiss_index_{platform}.index"
            faiss.write_index(index, str(index_path))
            print(f"   âœ… FAISS index oluÅŸturuldu ve kaydedildi: {platform} ({len(platform_embeddings)} vectors)")
    
    def _extract_platform(self, component: str) -> str:
        """Extract platform from component"""
        if not isinstance(component, str):
            return "unknown"
        component_lower = component.lower()
        if "android" in component_lower:
            return "android"
        elif "ios" in component_lower or "iphone" in component_lower or "ipad" in component_lower:
            return "ios"
        else:
            return "unknown"
    
    def _extract_language(self, lang: str) -> str:
        """Extract language code from language column"""
        if not isinstance(lang, str):
            return "unknown"
        # Extract language code (e.g., "en (0.75)" -> "en")
        match = re.match(r'([a-z]{2})', lang.lower())
        if match:
            return match.group(1)
        return "unknown"
    
    def _normalize_version(self, version: str) -> Tuple[int, int, int]:
        """Normalize version string to tuple of integers"""
        if not isinstance(version, str) or version == 'N/A' or version == '':
            return (0, 0, 0)
        try:
            # Extract numbers from version string
            parts = re.findall(r'\d+', version)
            if not parts:
                return (0, 0, 0)
            # Pad with zeros to get (major, minor, patch)
            parts = [int(p) for p in parts[:3]]
            while len(parts) < 3:
                parts.append(0)
            return tuple(parts[:3])
        except:
            return (0, 0, 0)
    
    def _calculate_version_similarity(self, query_version: str, result_version: str) -> float:
        """Calculate version similarity (0.0-1.0)"""
        if not query_version or query_version == 'N/A' or not result_version or result_version == 'N/A':
            return 0.0
        
        query_v = self._normalize_version(query_version)
        result_v = self._normalize_version(result_version)
        
        # Exact match
        if query_v == result_v:
            return 1.0
        
        # Major version match
        if query_v[0] == result_v[0] and query_v[0] > 0:
            # Minor version match
            if query_v[1] == result_v[1]:
                # Patch difference
                patch_diff = abs(query_v[2] - result_v[2])
                return 0.9 - (patch_diff * 0.05)  # 0.9, 0.85, 0.8, ...
            else:
                # Minor difference
                minor_diff = abs(query_v[1] - result_v[1])
                return 0.7 - (minor_diff * 0.1)  # 0.7, 0.6, 0.5, ...
        else:
            # Major version mismatch
            return 0.0
    
    def search(
        self,
        query: str,
        application: str = None,
        platform: str = None,
        version: str = None,
        language: str = None,
        top_k: int = 10,
        selected_columns: List[str] = None
    ) -> List[Dict]:
        """
        Hybrid search: FAISS (fast candidate retrieval) + Cross-Encoder (accurate re-ranking)
        
        Args:
            query: Query text to search for
            application: Application filter ('BiP', 'TV+', 'Fizy', etc.)
            platform: Platform filter ('android', 'ios', or None)
            version: Version filter (e.g., '3.70.19')
            language: Language filter (e.g., 'tr', 'en')
            top_k: Number of results to return
            selected_columns: Columns to use for cross-encoder comparison (default: ['Summary', 'Description'])
        
        Returns:
            List of result dictionaries with scores and metadata
        """
        # Default columns if not specified
        if selected_columns is None:
            selected_columns = ['Summary', 'Description']
        print(f"\nğŸ” Hybrid Search baÅŸlÄ±yor...")
        print(f"   Query: {query}")
        if application:
            print(f"   Application: {application}")
        if platform:
            print(f"   Platform: {platform}")
        if version:
            print(f"   Version: {version}")
        if language:
            print(f"   Language: {language}")
        
        start_time = time.time()
        
        # ========================================
        # STAGE 1: FAISS - Fast Candidate Retrieval
        # ========================================
        print(f"\nâš¡ STAGE 1: FAISS ile hÄ±zlÄ± candidate bulma...")
        stage1_start = time.time()
        
        # Apply initial filters (application, language)
        filtered_df = self.df.copy()
        filtered_indices = filtered_df.index.tolist()
        
        if application:
            filtered_df = filtered_df[filtered_df['Application'] == application]
            filtered_indices = filtered_df.index.tolist()
            print(f"   âœ“ Application filter: {len(filtered_df)} kayÄ±t")
        
        if language and self.df['Language'].notna().any():
            filtered_df = filtered_df[filtered_df['Language'] == language]
            filtered_indices = filtered_df.index.tolist()
            print(f"   âœ“ Language filter: {len(filtered_df)} kayÄ±t")
        
        if len(filtered_df) == 0:
            print("   âš ï¸ Filtre sonrasÄ± hiÃ§ kayÄ±t kalmadÄ±")
            return []
        
        # Encode query
        query_embedding = self.bi_encoder.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_embedding)
        
        # Search in appropriate FAISS index
        if platform and platform in self.faiss_indices:
            # Use platform-specific index
            index = self.faiss_indices[platform]
            platform_df = self.df[self.df['Platform'] == platform]
            
            # Further filter by application and language
            if application:
                platform_df = platform_df[platform_df['Application'] == application]
            if language and self.df['Language'].notna().any():
                platform_df = platform_df[platform_df['Language'] == language]
            
            if len(platform_df) == 0:
                print("   âš ï¸ Platform + filters sonrasÄ± hiÃ§ kayÄ±t kalmadÄ±")
                return []
            
            # Get candidates (limited by available records)
            k = min(self.n_candidates, len(platform_df))
            print(f"   ğŸ” {platform} FAISS index'te arama yapÄ±lÄ±yor (top {k} candidate)...")
            
            # Search FAISS
            distances, indices = index.search(query_embedding.astype('float32'), k)
            
            # Map FAISS indices to dataframe indices
            platform_indices = platform_df.index.tolist()
            candidate_indices = [platform_indices[i] for i in indices[0] if i < len(platform_indices)]
            candidate_df = self.df.loc[candidate_indices]
            
        else:
            # Search across all platforms (slower but comprehensive)
            print(f"   ğŸ” TÃ¼m FAISS indices'te arama yapÄ±lÄ±yor...")
            all_candidates = []
            
            for plat, index in self.faiss_indices.items():
                platform_df = self.df[self.df['Platform'] == plat]
                
                # Further filter
                if application:
                    platform_df = platform_df[platform_df['Application'] == application]
                if language and self.df['Language'].notna().any():
                    platform_df = platform_df[platform_df['Language'] == language]
                
                if len(platform_df) == 0:
                    continue
                
                k = min(self.n_candidates // len(self.faiss_indices), len(platform_df))
                distances, indices = index.search(query_embedding.astype('float32'), k)
                
                platform_indices = platform_df.index.tolist()
                for i, dist in zip(indices[0], distances[0]):
                    if i < len(platform_indices):
                        all_candidates.append({
                            'index': platform_indices[i],
                            'faiss_score': float(dist)
                        })
            
            # Sort by FAISS score and take top N
            all_candidates.sort(key=lambda x: x['faiss_score'], reverse=True)
            candidate_indices = [c['index'] for c in all_candidates[:self.n_candidates]]
            candidate_df = self.df.loc[candidate_indices]
        
        stage1_time = time.time() - stage1_start
        print(f"   âœ… STAGE 1 tamamlandÄ±: {len(candidate_df)} candidate bulundu ({stage1_time:.2f}s)")
        
        # ========================================
        # STAGE 2: Cross-Encoder - Accurate Re-ranking
        # ========================================
        print(f"\nğŸ¯ STAGE 2: Cross-Encoder ile hassas re-ranking...")
        print(f"   ğŸ“‹ KullanÄ±lan sÃ¼tunlar: {selected_columns}")
        stage2_start = time.time()
        
        # Prepare pairs for cross-encoder using selected columns
        # Combine text from selected columns that exist in the dataframe
        available_columns = [col for col in selected_columns if col in candidate_df.columns]
        
        if not available_columns:
            # Fallback to Summary if no columns available
            available_columns = ['Summary']
            print(f"   âš ï¸  SeÃ§ili sÃ¼tunlar bulunamadÄ±, Summary kullanÄ±lÄ±yor")
        
        # Combine text from all selected columns
        combined_texts = []
        for idx, row in candidate_df.iterrows():
            texts = [str(row[col]) for col in available_columns if pd.notna(row[col])]
            combined_text = ' '.join(texts)
            combined_texts.append(combined_text)
        
        pairs = [[query, text] for text in combined_texts]
        
        print(f"   ğŸ¤– Cross-encoder ile {len(pairs)} candidate kÄ±yaslanÄ±yor...")
        
        # Get cross-encoder scores
        cross_scores = self.cross_encoder.predict(pairs)
        
        stage2_time = time.time() - stage2_start
        print(f"   âœ… STAGE 2 tamamlandÄ± ({stage2_time:.2f}s)")
        
        # ========================================
        # STAGE 3: Final Scoring with Metadata
        # ========================================
        print(f"\nğŸ“Š STAGE 3: Final scoring (version, platform, language)...")
        
        results = []
        for idx, (df_idx, row) in enumerate(candidate_df.iterrows()):
            cross_score = float(cross_scores[idx])
            
            # Calculate version similarity if version is provided
            version_similarity = 0.0
            if version:
                version_similarity = self._calculate_version_similarity(version, row['App Version'])
            
            # Calculate platform similarity
            platform_similarity = 1.0 if platform and row['Platform'] == platform else 0.0
            
            # Calculate language similarity
            language_similarity = 1.0 if language and pd.notna(row['Language']) and row['Language'] == language else 0.0
            
            # Final score: weighted combination
            # Cross-encoder score is primary (70%)
            # Version similarity (15%)
            # Platform similarity (10%)
            # Language similarity (5%)
            final_score = (
                cross_score * 0.70 +
                version_similarity * 0.15 +
                platform_similarity * 0.10 +
                language_similarity * 0.05
            )
            
            # Clean values for JSON serialization (NaN not allowed in JSON)
            import math
            app_version_clean = row['App Version'] if pd.notna(row['App Version']) else 'N/A'
            priority_clean = row.get('Priority', 'Unknown')
            if pd.isna(priority_clean):
                priority_clean = 'Unknown'
            
            results.append({
                'index': int(df_idx),
                'summary': str(row['Summary']),
                'description': row['Description'][:200] + '...' if len(str(row['Description'])) > 200 else str(row['Description']),
                'application': str(row.get('Application', 'Unknown')),
                'platform': str(row['Platform']) if pd.notna(row['Platform']) else 'unknown',
                'app_version': str(app_version_clean),
                'language': str(row['Language']) if pd.notna(row['Language']) else None,
                'priority': str(priority_clean),
                'cross_encoder_score': float(cross_score) if not math.isnan(cross_score) else 0.0,
                'version_similarity': float(version_similarity),
                'platform_similarity': float(platform_similarity),
                'language_similarity': float(language_similarity),
                'final_score': float(final_score) if not math.isnan(final_score) else 0.0
            })
        
        # Sort by final score
        results.sort(key=lambda x: x['final_score'], reverse=True)
        
        total_time = time.time() - start_time
        print(f"   âœ… STAGE 3 tamamlandÄ±")
        print(f"\nâ±ï¸  TOPLAM SÃœRE: {total_time:.2f}s (Stage 1: {stage1_time:.2f}s, Stage 2: {stage2_time:.2f}s)")
        
        return results[:top_k]
    
    def display_results(
        self,
        results: List[Dict],
        query: str,
        application: str = None,
        platform: str = None,
        version: str = None,
        language: str = None
    ):
        """Display search results in a formatted way"""
        print("\n" + "=" * 100)
        print("ğŸ¯ HYBRID SEARCH SONUÃ‡LARI")
        print("=" * 100)
        print(f"ğŸ“ Query: {query}")
        if application:
            print(f"ğŸ“± Application: {application}")
        if platform:
            print(f"ğŸ“± Platform: {platform}")
        if version:
            print(f"ğŸ”¢ Version: {version}")
        if language:
            print(f"ğŸŒ Language: {language}")
        print(f"âœ… {len(results)} sonuÃ§ bulundu")
        print("=" * 100)
        
        for i, result in enumerate(results, 1):
            # Determine match quality based on final score
            if result['final_score'] >= 4.0:
                quality = "âœ… EXCELLENT MATCH"
            elif result['final_score'] >= 3.0:
                quality = "ğŸ‘ VERY GOOD MATCH"
            elif result['final_score'] >= 2.0:
                quality = "ğŸ” GOOD MATCH"
            elif result['final_score'] >= 1.0:
                quality = "âš ï¸ MODERATE MATCH"
            else:
                quality = "â“ WEAK MATCH"
            
            print(f"\n[{i}] {quality} - Score: {result['final_score']:.4f}")
            print(f"    ğŸ“‹ Summary: {result['summary']}")
            print(f"    ğŸ“„ Description: {result['description']}")
            print(f"    ğŸ“± Application: {result['application']}")
            print(f"    ğŸ“± Platform: {result['platform']}")
            print(f"    ğŸ”¢ App Version: {result['app_version']}")
            print(f"    ğŸŒ Language: {result['language']}")
            print(f"    âš¡ Priority: {result['priority']}")
            print(f"    ğŸ“Š Cross-Encoder Score: {result['cross_encoder_score']:.4f}")
            if result['version_similarity'] > 0:
                print(f"    ğŸ“Š Version Similarity: {result['version_similarity']:.4f}")
            if result['platform_similarity'] > 0:
                print(f"    ğŸ“Š Platform Match: âœ“")
            if result['language_similarity'] > 0:
                print(f"    ğŸ“Š Language Match: âœ“")
            print(f"    ğŸ”— Index: {result['index']}")
        
        print("\n" + "=" * 100)


def get_user_input():
    """Get search parameters from user"""
    print("\n" + "=" * 100)
    print("ğŸ“ ARAMA BÄ°LGÄ°LERÄ°NÄ° GÄ°RÄ°N")
    print("=" * 100)
    
    query = input("ğŸ” Arama cÃ¼mlesi: ").strip()
    if not query:
        print("âŒ Arama cÃ¼mlesi boÅŸ olamaz!")
        return None
    
    application = input("ğŸ“± Uygulama (BiP/TV+/Fizy/HesabÄ±m/LifeBox/Paycell/Dergilik, boÅŸ bÄ±rakabilirsiniz): ").strip()
    if application and application not in ['BiP', 'TV+', 'Fizy', 'HesabÄ±m', 'LifeBox', 'Paycell', 'Dergilik']:
        print("âš ï¸ GeÃ§ersiz uygulama, filtre uygulanmayacak")
        application = None
    
    platform = input("ğŸ“± Platform (android/ios, boÅŸ bÄ±rakabilirsiniz): ").strip().lower()
    if platform and platform not in ['android', 'ios']:
        print("âš ï¸ GeÃ§ersiz platform, filtre uygulanmayacak")
        platform = None
    
    version = input("ğŸ”¢ SÃ¼rÃ¼m (Ã¶rn: 3.70.19, boÅŸ bÄ±rakabilirsiniz): ").strip()
    if not version:
        version = None
    
    language = input("ğŸŒ Dil (tr/en, boÅŸ bÄ±rakabilirsiniz): ").strip().lower()
    if language and len(language) != 2:
        print("âš ï¸ GeÃ§ersiz dil kodu, filtre uygulanmayacak")
        language = None
    
    top_k_str = input("ğŸ”¢ KaÃ§ sonuÃ§ gÃ¶sterilsin? (varsayÄ±lan: 10): ").strip()
    try:
        top_k = int(top_k_str) if top_k_str else 10
    except:
        top_k = 10
    
    return {
        'query': query,
        'application': application,
        'platform': platform,
        'version': version,
        'language': language,
        'top_k': top_k
    }


def main():
    """Main function"""
    print("\n" + "=" * 100)
    print("ğŸš€ HYBRID SEARCH SYSTEM (FAISS + Cross-Encoder)")
    print("=" * 100)
    print("Bu sistem 2-aÅŸamalÄ± arama yapar:")
    print("  1ï¸âƒ£ STAGE 1: FAISS ile hÄ±zlÄ± candidate bulma (~0.5s)")
    print("  2ï¸âƒ£ STAGE 2: Cross-Encoder ile hassas sÄ±ralama (~2s)")
    print("  3ï¸âƒ£ STAGE 3: Version, platform, language skorlamasÄ±")
    print("")
    print("âš¡ 4x daha hÄ±zlÄ±, %97 accuracy!")
    print("=" * 100)
    
    try:
        # Initialize search system
        search_system = HybridSearch()
        
        while True:
            # Get user input
            params = get_user_input()
            if params is None:
                continue
            
            # Check for exit
            if params['query'].lower() in ['exit', 'quit', 'Ã§Ä±kÄ±ÅŸ']:
                print("\nğŸ‘‹ Ã‡Ä±kÄ±ÅŸ yapÄ±lÄ±yor...")
                break
            
            # Perform search
            results = search_system.search(
                query=params['query'],
                application=params['application'],
                platform=params['platform'],
                version=params['version'],
                language=params['language'],
                top_k=params['top_k']
            )
            
            # Display results
            search_system.display_results(
                results=results,
                query=params['query'],
                application=params['application'],
                platform=params['platform'],
                version=params['version'],
                language=params['language']
            )
            
            # Ask if user wants to continue
            continue_search = input("\nğŸ”„ BaÅŸka bir arama yapmak ister misiniz? (evet/hayÄ±r): ").strip().lower()
            if continue_search not in ['evet', 'yes', 'e', 'y']:
                print("\nğŸ‘‹ Ã‡Ä±kÄ±ÅŸ yapÄ±lÄ±yor...")
                break
    
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Ã‡Ä±kÄ±ÅŸ yapÄ±lÄ±yor...")
    except Exception as e:
        print(f"\nâŒ Hata: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

